{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def group_text_into_line(list_text_xyxy, alpha=1.0, debug=False):\n",
    "    \"\"\"\n",
    "    list_text_xyxy = [ (text, (x0, y0, x1, y1)), ... ]\n",
    "    \"\"\"\n",
    "    uid_to_word_map = {}\n",
    "    for uid, (text, (x0,y0,x1,y1)) in enumerate(list_text_xyxy):\n",
    "        uid_to_word_map[uid] = (text, (x0,y0,x1,y1))\n",
    "\n",
    "    #================================================================\n",
    "    seqs = {} # text sequences\n",
    "    for uid in uid_to_word_map.keys():\n",
    "        text, (x0, y0, x1, y1) = uid_to_word_map[uid]\n",
    "        cx = 0.5*(x0 + x1)\n",
    "        dw = (x1 - x0) / len(text) * alpha\n",
    "        x0, y0, x1, y1 = x0-dw, y0, x1+dw, y1\n",
    "        if debug:\n",
    "            print(f'uid:text = {uid},:{text}')\n",
    "        \n",
    "        attach_to_head, attach_to_tail = None, None\n",
    "        #--- start to check for each text group ------------------------------\n",
    "        for uid0, node0 in seqs.items(): # O(N^2) in the worse case\n",
    "            first_uid = node0['uid']\n",
    "            last_uid = node0['last']['uid']\n",
    "    \n",
    "            #-------------------------------------------------\n",
    "            text_f, (x0_f, y0_f, x1_f, y1_f) = uid_to_word_map[first_uid]\n",
    "            cx_f = 0.5*(x0_f + x1_f)\n",
    "            dw_f = (x1_f - x0_f) / len(text_f) * alpha\n",
    "            x0_f, y0_f, x1_f, y1_f = x0_f-dw_f, y0_f, x1_f+dw_f, y1_f\n",
    "            \n",
    "            intersection_area_f = max(0, min(x1, x1_f) - max(x0, x0_f)) * max(0, min(y1, y1_f) - max(y0, y0_f))\n",
    "            \n",
    "            if intersection_area_f > 0:\n",
    "                if (first_uid != last_uid):\n",
    "                    attach_to_head = uid0\n",
    "                else:\n",
    "                    if cx <= cx_f:\n",
    "                        attach_to_head = uid0\n",
    "                    else:\n",
    "                        attach_to_tail = uid0\n",
    "\n",
    "            #-------------------------------------------------\n",
    "            text_l, (x0_l, y0_l, x1_l, y1_l) = uid_to_word_map[last_uid]\n",
    "            dw_l = (x1_l - x0_l) / len(text_l) * alpha\n",
    "            x0_l, y0_l, x1_l, y1_l = x0_l-dw_l, y0_l, x1_l+dw_l, y1_l\n",
    "\n",
    "            intersection_area_l = max(0, min(x1, x1_l) - max(x0, x0_l)) * max(0, min(y1, y1_l) - max(y0, y0_l))\n",
    "\n",
    "            if (first_uid != last_uid) and (intersection_area_l > 0):\n",
    "                attach_to_tail = uid0\n",
    "\n",
    "            if debug:\n",
    "                print(f'> uid0:text = {first_uid}:{text_f}, uid1:text = {last_uid}:{text_l}')\n",
    "                print(f'> attach_to_head = {attach_to_head}, attach_to_tail = {attach_to_tail}')\n",
    "\n",
    "            \n",
    "            if (attach_to_head is not None) and (attach_to_tail is not None):\n",
    "                assert attach_to_head != attach_to_tail\n",
    "                break\n",
    "\n",
    "        #-------------------------------------------------\n",
    "        if attach_to_head is not None:\n",
    "            node0 = seqs[attach_to_head]\n",
    "\n",
    "            new = {'uid':uid} # create a new node\n",
    "            new['next'] = node0 # now, the new node is the first node\n",
    "            new['last'] = node0['last'] # the first node is responsible for keeping the information about 'last'\n",
    "            del node0['last'] # this is no longer the first node and so don't need to keep the information about 'last'\n",
    "            \n",
    "            seqs[uid] = new\n",
    "            del seqs[attach_to_head]\n",
    "            \n",
    "        #-------------------------------------------------\n",
    "        if attach_to_tail is not None:\n",
    "            node0 = seqs[attach_to_tail]\n",
    "\n",
    "            if uid not in seqs:\n",
    "                new = {'uid':uid, 'next':None} # create a new node\n",
    "                node0['last']['next'] = new\n",
    "                node0['last'] = new\n",
    "                if debug:\n",
    "                    print(f'> attach_to_tail --> scenario 1')\n",
    "            else:\n",
    "                old = seqs[uid]\n",
    "                node0['last']['next'] = old\n",
    "                node0['last'] = old['last']\n",
    "                del seqs[uid]\n",
    "                if debug:\n",
    "                    print(f'> attach_to_tail --> scenario 2')\n",
    "\n",
    "        #--- if there is no overlap, then create a new seqs item -----------------------\n",
    "        if (attach_to_head is None) and (attach_to_tail is None):\n",
    "            new = {'uid':uid, 'next':None}\n",
    "            new['last'] = new # the last node is itself, because there is only one node\n",
    "            seqs[uid] = new\n",
    "\n",
    "    #=======================================================================\n",
    "    i_to_line_map = {}\n",
    "    for i, (uid, node) in enumerate(seqs.items()):\n",
    "        assert uid == node['uid']\n",
    "        arr = []\n",
    "        x0_min = float('inf')\n",
    "        y0_min = float('inf')\n",
    "        x1_max = float('-inf')\n",
    "        y1_max = float('-inf')\n",
    "        \n",
    "        while node is not None:\n",
    "            # print(t, end=' ::: ')\n",
    "            t, (x0,y0,x1,y1) = uid_to_word_map[node['uid']]\n",
    "            arr.append(t)\n",
    "            x0_min = min(x0_min, x0)\n",
    "            y0_min = min(y0_min, y0)\n",
    "            x1_max = max(x1_max, x1)\n",
    "            y1_max = max(y1_max, y1)\n",
    "            \n",
    "            node = node['next']\n",
    "        # print(f' <<{i+1}>> ')\n",
    "        \n",
    "        line_data = (' '.join(arr), (x0_min, y0_min, x1_max, y1_max))\n",
    "        i_to_line_map[i] = line_data\n",
    "\n",
    "    return i_to_line_map, seqs, uid_to_word_map\n",
    "\n",
    "def find_single_passage(seqs, uid_to_word_map, delimiter='\\t'):\n",
    "    def find_x_center(node):\n",
    "        _, (x0, _, _, _) = uid_to_word_map[node['uid']] # first node of a sequence\n",
    "        _, (_, _, x1, _) = uid_to_word_map[node['last']['uid']] # last node of a sequence\n",
    "        return 0.5*(x0 + x1)\n",
    "        \n",
    "    #-------------------------------------------------------------------\n",
    "    longlines = {}\n",
    "    for i, node in enumerate(seqs.values()):\n",
    "        _, (_, y0_f, _, y1_f) = uid_to_word_map[node['uid']] # first node of a sequence\n",
    "        _, (_, y0_l, _, y1_l) = uid_to_word_map[node['last']['uid']] # last node of a sequence\n",
    "        y0 = min(y0_f, y0_l)\n",
    "        y1 = max(y1_f, y1_l)\n",
    "        \n",
    "        found = False\n",
    "        for lline in longlines.values(): # O(n^2) time complexity in the worst case\n",
    "            yy0 = lline['y0']\n",
    "            yy1 = lline['y1']\n",
    "            \n",
    "            if max(y0, yy0) < min(y1, yy1): # if the lines are overlaping \n",
    "                lline['stack'].append(node) # this que will be sorted later\n",
    "                lline['y0'] = min(y0, yy0) # update x0\n",
    "                lline['y1'] = max(y1, yy1) # update x1\n",
    "                found = True\n",
    "                break # if found, then exit from the for loop, because there is no need to find any more\n",
    "            \n",
    "        if not found:\n",
    "            longlines[i] = {'stack':[node], 'y0':y0, 'y1':y1} # initialization\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    passage = []\n",
    "    for lline in longlines.values(): # sort in x direction\n",
    "        stack_sorted = sorted(lline['stack'], key=lambda node: find_x_center(node))\n",
    "        \n",
    "        sentences, x0, x1 = [], float('inf'), float('-inf')\n",
    "        for node in stack_sorted:\n",
    "            sentence = []\n",
    "            while node:\n",
    "                text, (xx0, yy0, xx1, yy1) = uid_to_word_map[node['uid']]\n",
    "                sentence.append(text)\n",
    "                x0 = min(x0, xx0)\n",
    "                x1 = max(x1, xx1)\n",
    "                node = node['next']\n",
    "            sentences.append(' '.join(sentence))\n",
    "        passage.append([delimiter.join(sentences), (x0, lline['y0'], x1, lline['y1'])])\n",
    "    \n",
    "    passage = sorted(passage, key=lambda s: 0.5*(s[-1][1] + s[-1][3])) # 0.5*(y0 + y1) sort in x direction\n",
    "\n",
    "    return passage, longlines\n",
    "\n",
    "\n",
    "def find_index_left_right_most(idx_source, i_to_line_map):\n",
    "    \"\"\"\n",
    "    To find the left or right most text, heap queue is used.\n",
    "    \"\"\"\n",
    "    _, (x0,y0,x1,y1) = i_to_line_map[idx_source]\n",
    "    cx = 0.5*(x0 + x1)\n",
    "    cy = 0.5*(y0 + y1)\n",
    "    \n",
    "    left, right = [], [] # don't neet to do 'heapq.heapify()', because it's empty\n",
    "    for j, (_, (xx0,yy0,xx1,yy1)) in i_to_line_map.items():\n",
    "        if (j != idx_source) and (yy0 <= cy <= yy1):\n",
    "            cxx = 0.5*(xx0 + xx1)\n",
    "            if cx < cxx:\n",
    "                heapq.heappush(right, (cxx, j))\n",
    "            elif cxx < cx:\n",
    "                heapq.heappush(left, (-cxx, j))\n",
    "            else:\n",
    "                raise(Exception('Brad error: unknown situation encountered...'))\n",
    "\n",
    "    idx_left = left[0][1] if left else None\n",
    "    idx_right = right[0][1] if right else None\n",
    "    \n",
    "    return {'idx_left':idx_left, 'idx_right':idx_right}\n",
    "\n",
    "### TEST ################################################################################\n",
    "if __name__=='__main__':\n",
    "    from PIL import Image\n",
    "    from faster_rcnn import plot_text_xyxy\n",
    "\n",
    "    img = np.array(Image.open(r'D:\\codes\\OCR\\metrics_team2.JPG'))\n",
    "\n",
    "    # out = {'pred_texts': [('PERMANENT', (95, 610, 187, 623)),.......\n",
    "\n",
    "\n",
    "    list_text_xyxy = out['pred_texts']\n",
    "\n",
    "    i_to_line_map, seqs, uid_to_word_map = group_text_into_line(list_text_xyxy, alpha=1.0, debug=False)\n",
    "    passage, longlines = find_single_passage(seqs, uid_to_word_map, delimiter='\\t')\n",
    "    # passage, longlines = find_single_passage(seqs, uid_to_word_map, delimiter=';')\n",
    "\n",
    "    print('\\n'.join(p[0] for p in passage)) # text extracted\n",
    "\n",
    "    plot_text_xyxy(img, i_to_line_map.values())\n",
    "\n",
    "    plot_text_xyxy(img, passage)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     from brad_text import Word_Source_Dictionary\n",
    "\n",
    "#     wsd = Word_Source_Dictionary()\n",
    "\n",
    "#     for source, (text, _)  in i_to_line_map.items():\n",
    "#         wsd.add(text, source) # create word-source dictionary\n",
    "\n",
    "#     # print(wsd.find_sources(['venezuela'])) # 'apple' AND 'banana'\n",
    "#     # print(wsd.find_sources(['apple', 'door'])) # 'apple' OR 'door'\n",
    "#     # print(wsd.find_sources(['apple door'])) # 'apple' AND 'door'\n",
    "\n",
    "#     # print(wsd.find_sources(['pp']))\n",
    "#     # print(wsd.find_sources(['app'], allow_prefix=False, allow_suffix=False)) # exact matching\n",
    "#     # print(wsd.find_sources(['apple']))\n",
    "#     # print(wsd.find_sources(['apple'], allow_prefix=False, allow_suffix=False, allow_inner_suffix=True, allow_inner_prefix=True))\n",
    "#     # print(wsd.find_sources(['apple'], allow_prefix=True, allow_suffix=True, allow_inner_suffix=True, allow_inner_prefix=True))\n",
    "\n",
    "#     ii = wsd.find_sources(['china'])\n",
    "#     # ii = wsd.find_sources(['US'], allow_prefix=False, allow_suffix=False) # exact matching\n",
    "#     for i in ii:\n",
    "#         print(i, i_to_line_map[i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
