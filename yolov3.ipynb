{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "- [Github] https://github.com/WongKinYiu/yolov7\n",
    "- [Github] https://github.com/pjreddie/darknet\n",
    "- [Github] https://github.com/ayooshkathuria/pytorch-yolo-v3?tab=readme-ov-file\n",
    "- [Github] https://github.com/eriklindernoren/PyTorch-YOLOv3\n",
    "- [Github] https://github.com/ayooshkathuria/pytorch-yolo-v3?tab=readme-ov-file\n",
    "\n",
    "- [PAPER 2024] A COMPREHENSIVE REVIEW OF YOLO ARCHITECTURES IN COMPUTER VISION: FROM YOLOV1 TO YOLOV8 AND YOLO-NAS: https://arxiv.org/pdf/2304.00501\n",
    "- [PAPER 2018] YOLOv3: An Incremental Improvement: https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "- [PAPER 2016] YOLO9000: Better, Faster, Stronger: https://arxiv.org/pdf/1612.08242\n",
    "- [PAPER 2016] You Only Look Once: Unified, Real-Time Object Detection: https://arxiv.org/pdf/1506.02640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import argparse\n",
    "import tqdm\n",
    "from itertools import chain\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import time\n",
    "import platform\n",
    "import subprocess\n",
    "import random\n",
    "import datetime\n",
    "import glob\n",
    "import warnings\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('pip install imgaug')\n",
    "os.system('pip install terminaltables')\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_determinism(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    ia.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def worker_seed_set(worker_id):\n",
    "    # See for details of numpy:\n",
    "    # https://github.com/pytorch/pytorch/issues/5059#issuecomment-817392562\n",
    "    # See for details of random:\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html#dataloader\n",
    "\n",
    "    # NumPy\n",
    "    uint64_seed = torch.initial_seed()\n",
    "    ss = np.random.SeedSequence([uint64_seed])\n",
    "    np.random.seed(ss.generate_state(4))\n",
    "\n",
    "    # random\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def to_cpu(tensor):\n",
    "    return tensor.detach().cpu()\n",
    "\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as fp:\n",
    "        names = fp.read().splitlines()\n",
    "    return names\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def rescale_boxes(boxes, current_dim, original_shape):\n",
    "    \"\"\"\n",
    "    Rescales bounding boxes to the original shape\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
    "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
    "\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = current_dim - pad_y\n",
    "    unpad_w = current_dim - pad_x\n",
    "\n",
    "    # Rescale bounding boxes to dimension of original image\n",
    "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
    "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy_np(x):\n",
    "    y = np.zeros_like(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (list).\n",
    "        conf:  Objectness value from 0-1 (list).\n",
    "        pred_cls: Predicted object classes (list).\n",
    "        target_cls: True object classes (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    ap, p, r = [], [], []\n",
    "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 and n_gt == 0:\n",
    "            continue\n",
    "        elif n_p == 0 or n_gt == 0:\n",
    "            ap.append(0)\n",
    "            r.append(0)\n",
    "            p.append(0)\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum()\n",
    "            tpc = (tp[i]).cumsum()\n",
    "\n",
    "            # Recall\n",
    "            recall_curve = tpc / (n_gt + 1e-16)\n",
    "            r.append(recall_curve[-1])\n",
    "\n",
    "            # Precision\n",
    "            precision_curve = tpc / (tpc + fpc)\n",
    "            p.append(precision_curve[-1])\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            ap.append(compute_ap(recall_curve, precision_curve))\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def get_batch_statistics(outputs, targets, iou_threshold):\n",
    "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
    "    batch_metrics = []\n",
    "    for sample_i in range(len(outputs)):\n",
    "\n",
    "        if outputs[sample_i] is None:\n",
    "            continue\n",
    "\n",
    "        output = outputs[sample_i]\n",
    "        pred_boxes = output[:, :4]\n",
    "        pred_scores = output[:, 4]\n",
    "        pred_labels = output[:, -1]\n",
    "\n",
    "        true_positives = np.zeros(pred_boxes.shape[0])\n",
    "\n",
    "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
    "        target_labels = annotations[:, 0] if len(annotations) else []\n",
    "        if len(annotations):\n",
    "            detected_boxes = []\n",
    "            target_boxes = annotations[:, 1:]\n",
    "\n",
    "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "\n",
    "                # If targets are found break\n",
    "                if len(detected_boxes) == len(annotations):\n",
    "                    break\n",
    "\n",
    "                # Ignore if label is not one of the target labels\n",
    "                if pred_label not in target_labels:\n",
    "                    continue\n",
    "\n",
    "                # Filter target_boxes by pred_label so that we only match against boxes of our own label\n",
    "                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))\n",
    "\n",
    "                # Find the best matching target for our predicted box\n",
    "                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)\n",
    "\n",
    "                # Remap the index in the list of filtered targets for that label to the index in the list with all targets.\n",
    "                box_index = filtered_target_position[box_filtered_index]\n",
    "\n",
    "                # Check if the iou is above the min treshold and i\n",
    "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
    "                    true_positives[pred_i] = 1\n",
    "                    detected_boxes += [box_index]\n",
    "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
    "    return batch_metrics\n",
    "\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = \\\n",
    "            box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = \\\n",
    "            box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) -\n",
    "             torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    # iou = inter / (area1 + area2 - inter)\n",
    "    return inter / (area1[:, None] + area2 - inter)\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None):\n",
    "    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n",
    "    Returns:\n",
    "         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "\n",
    "    # Settings\n",
    "    # (pixels) minimum and maximum box width and height\n",
    "    max_wh = 4096\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 1.0  # seconds to quit after\n",
    "    multi_label = nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=\"cpu\")] * prediction.shape[0]\n",
    "\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[x[..., 4] > conf_thres]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            # sort by confidence\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        # boxes (offset by class), scores\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "\n",
    "        output[xi] = to_cpu(x[i])\n",
    "\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def print_environment_info():\n",
    "    \"\"\"\n",
    "    Prints infos about the environment and the system.\n",
    "    This should help when people make issues containg the printout.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Environment information:\")\n",
    "\n",
    "    # Print OS information\n",
    "    print(f\"System: {platform.system()} {platform.release()}\")\n",
    "\n",
    "    # Print poetry package version\n",
    "    try:\n",
    "        print(f\"Current Version: {subprocess.check_output(['poetry', 'version'], stderr=subprocess.DEVNULL).decode('ascii').strip()}\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"Not using the poetry package\")\n",
    "\n",
    "    # Print commit hash if possible\n",
    "    try:\n",
    "        print(f\"Current Commit Hash: {subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], stderr=subprocess.DEVNULL).decode('ascii').strip()}\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"No git or repo found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This new loss function is based on https://github.com/ultralytics/yolov3/blob/master/utils/loss.py\n",
    "\n",
    "\n",
    "# def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9):\n",
    "def bbox_iou_FOR_LOSS(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9): # Brad\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    box2 = box2.T\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    iou = inter / union\n",
    "    if GIoU or DIoU or CIoU:\n",
    "        # convex (smallest enclosing box) width\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
    "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
    "            if DIoU:\n",
    "                return iou - rho2 / c2  # DIoU\n",
    "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * \\\n",
    "                    torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / ((1 + eps) - iou + v)\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "            c_area = cw * ch + eps  # convex area\n",
    "            return iou - (c_area - union) / c_area  # GIoU\n",
    "    else:\n",
    "        return iou  # IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(p, targets, model): # p: [(B, C, H//32, W//32, 1 + 4 num_class), (B, C, H//16, W//16, 1 + 4 num_class), (B, C, H//8, W//8, 1 + 4 num_class)]\n",
    "    # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n",
    "    # Brad: targets (nt,6)\n",
    "    na, nt = 3, targets.shape[0]  # number of anchors, targets #TODO\n",
    "    tcls, tbox, indices, anch = [], [], [], []\n",
    "    gain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\n",
    "    # Make a tensor that iterates 0-2 for 3 anchors and repeat that as many times as we have target boxes\n",
    "    ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt) # (na,nt): e.g. [[0,0,...,0], [1,1,...,1], [2,2,...,2]]\n",
    "    # Copy target boxes anchor size times and append an anchor index to each copy the anchor index is also expressed by the new first dimension\n",
    "    targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2) # (na,nt,6+1): targets.shape = (nt,6); targets.repeat(na,1,1).shape = (na,nt,6); ai[:, :, None].shape = (na,nt,1)\n",
    "\n",
    "    for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "        # Scale anchors by the yolo grid cell size so that an anchor with the size of the cell would result in 1\n",
    "        anchors = yolo_layer.anchors / yolo_layer.stride\n",
    "        # Add the number of yolo cells in this layer the gain tensor\n",
    "        # The gain tensor matches the collums of our targets (img id, class, x, y, w, h, anchor id)\n",
    "        gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain; gain = [1, 1, W//n, H//n, W//n, H//n, 1], i.e. W//n, H//n = feature map size\n",
    "        # Scale targets by the number of yolo layer cells, they are now in the yolo cell coordinate system\n",
    "        t = targets * gain\n",
    "        # Check if we have targets\n",
    "        if nt:\n",
    "            # Calculate ration between anchor and target box for both width and height\n",
    "            r = t[:, :, 4:6] / anchors[:, None]\n",
    "            # Select the ratios that have the highest divergence in any axis and check if the ratio is less than 4\n",
    "            j = torch.max(r, 1. / r).max(2)[0] < 4  # compare #TODO\n",
    "            # Only use targets that have the correct ratios for their anchors\n",
    "            # That means we only keep ones that have a matching anchor and we loose the anchor dimension\n",
    "            # The anchor id is still saved in the 7th value of each target\n",
    "            t = t[j]\n",
    "        else:\n",
    "            t = targets[0]\n",
    "\n",
    "        # Extract image id in batch and class id\n",
    "        b, c = t[:, :2].long().T\n",
    "        # We isolate the target cell associations.\n",
    "        # x, y, w, h are allready in the cell coordinate system meaning an x = 1.2 would be 1.2 times cellwidth\n",
    "        gxy = t[:, 2:4]\n",
    "        gwh = t[:, 4:6]  # grid wh\n",
    "        # Cast to int to get an cell index e.g. 1.2 gets associated to cell 1\n",
    "        gij = gxy.long()\n",
    "        # Isolate x and y index dimensions\n",
    "        gi, gj = gij.T  # grid xy indices\n",
    "\n",
    "        # Convert anchor indexes to int\n",
    "        a = t[:, 6].long()\n",
    "        # Add target tensors for this yolo layer to the output lists\n",
    "        # Add to index list and limit index range to prevent out of bounds\n",
    "        indices.append((b, a, gj.clamp_(0, gain[3].long() - 1), gi.clamp_(0, gain[2].long() - 1)))\n",
    "        # Add to target box list and convert box coordinates from global grid coordinates to local offsets in the grid cell\n",
    "        tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "        # Add correct anchor for each target to the list\n",
    "        anch.append(anchors[a])\n",
    "        # Add class for each target to the list\n",
    "        tcls.append(c)\n",
    "\n",
    "    return tcls, tbox, indices, anch\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     na = 3\n",
    "#     nt = 5\n",
    "#     targets = torch.arange(nt).view(nt,1).repeat(1,6)\n",
    "#     print(f'targets = {targets}')\n",
    "#     ai = torch.arange(na).view(na,1).repeat(1,nt)\n",
    "#     print(f'ai = {ai}')\n",
    "#     torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)\n",
    "#     print(targets.repeat(na, 1, 1).shape)\n",
    "#     print(ai[:, :, None].shape)\n",
    "#     torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets, model): # predictions: [(B, C, H//32, W//32, 1 + 4 num_class), (B, C, H//16, W//16, 1 + 4 num_class), (B, C, H//8, W//8, 1 + 4 num_class)]\n",
    "    # Check which device was used\n",
    "    device = targets.device\n",
    "\n",
    "    # Add placeholder varables for the different losses\n",
    "    lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n",
    "\n",
    "    # Build yolo targets\n",
    "    tcls, tbox, indices, anchors = build_targets(predictions, targets, model)  # targets\n",
    "\n",
    "    # Define different loss functions classification\n",
    "    BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device=device))\n",
    "    BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], device=device))\n",
    "\n",
    "    # Calculate losses for each yolo layer\n",
    "    for layer_index, layer_predictions in enumerate(predictions):\n",
    "        # Get image ids, anchors, grid index i and j for each target in the current yolo layer\n",
    "        b, anchor, grid_j, grid_i = indices[layer_index]\n",
    "        # Build empty object target tensor with the same shape as the object prediction\n",
    "        tobj = torch.zeros_like(layer_predictions[..., 0], device=device)  # target obj\n",
    "        # Get the number of targets for this layer.\n",
    "        # Each target is a label box with some scaling and the association of an anchor box.\n",
    "        # Label boxes may be associated to 0 or multiple anchors. So they are multiple times or not at all in the targets.\n",
    "        num_targets = b.shape[0]\n",
    "        # Check if there are targets for this batch\n",
    "        if num_targets:\n",
    "            # Load the corresponding values from the predictions for each of the targets\n",
    "            ps = layer_predictions[b, anchor, grid_j, grid_i]\n",
    "\n",
    "            # Regression of the box\n",
    "            # Apply sigmoid to xy offset predictions in each cell that has a target\n",
    "            pxy = ps[:, :2].sigmoid()\n",
    "            # Apply exponent to wh predictions and multiply with the anchor box that matched best with the label for each cell that has a target\n",
    "            pwh = torch.exp(ps[:, 2:4]) * anchors[layer_index]\n",
    "            # Build box out of xy and wh\n",
    "            pbox = torch.cat((pxy, pwh), 1)\n",
    "            # Calculate CIoU or GIoU for each target with the predicted box for its cell + anchor\n",
    "            # iou = bbox_iou(pbox.T, tbox[layer_index], x1y1x2y2=False, CIoU=True)\n",
    "            iou = bbox_iou_FOR_LOSS(pbox.T, tbox[layer_index], x1y1x2y2=False, CIoU=True)\n",
    "            \n",
    "            # We want to minimize our loss so we and the best possible IoU is 1 so we take 1 - IoU and reduce it with a mean\n",
    "            lbox += (1.0 - iou).mean()  # iou loss\n",
    "\n",
    "            # Classification of the objectness\n",
    "            # Fill our empty object target tensor with the IoU we just calculated for each target at the targets position\n",
    "            tobj[b, anchor, grid_j, grid_i] = iou.detach().clamp(0).type(tobj.dtype)  # Use cells with iou > 0 as object targets\n",
    "\n",
    "            # Classification of the class\n",
    "            # Check if we need to do a classification (number of classes > 1)\n",
    "            if ps.size(1) - 5 > 1:\n",
    "                # Hot one class encoding\n",
    "                t = torch.zeros_like(ps[:, 5:], device=device)  # targets\n",
    "                t[range(num_targets), tcls[layer_index]] = 1\n",
    "                # Use the tensor to calculate the BCE loss\n",
    "                lcls += BCEcls(ps[:, 5:], t)  # BCE\n",
    "\n",
    "        # Classification of the objectness the sequel\n",
    "        # Calculate the BCE loss between the on the fly generated target and the network prediction\n",
    "        lobj += BCEobj(layer_predictions[..., 4], tobj) # obj loss\n",
    "\n",
    "    lbox *= 0.05\n",
    "    lobj *= 1.0\n",
    "    lcls *= 0.5\n",
    "\n",
    "    # Merge losses\n",
    "    loss = lbox + lobj + lcls\n",
    "\n",
    "    return loss, to_cpu(torch.cat((lbox, lobj, lcls, loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgAug(object):\n",
    "    def __init__(self, augmentations=[]):\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Unpack data\n",
    "        img, boxes = data\n",
    "\n",
    "        # Convert xywh to xyxy\n",
    "        boxes = np.array(boxes)\n",
    "        boxes[:, 1:] = xywh2xyxy_np(boxes[:, 1:])\n",
    "\n",
    "        # Convert bounding boxes to imgaug\n",
    "        bounding_boxes = BoundingBoxesOnImage(\n",
    "            [BoundingBox(*box[1:], label=box[0]) for box in boxes],\n",
    "            shape=img.shape)\n",
    "\n",
    "        # Apply augmentations\n",
    "        img, bounding_boxes = self.augmentations(image=img, bounding_boxes=bounding_boxes)\n",
    "\n",
    "        # Clip out of image boxes\n",
    "        bounding_boxes = bounding_boxes.clip_out_of_image()\n",
    "\n",
    "        # Convert bounding boxes back to numpy\n",
    "        boxes = np.zeros((len(bounding_boxes), 5))\n",
    "        for box_idx, box in enumerate(bounding_boxes):\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = box.x1\n",
    "            y1 = box.y1\n",
    "            x2 = box.x2\n",
    "            y2 = box.y2\n",
    "\n",
    "            # Returns (x, y, w, h)\n",
    "            boxes[box_idx, 0] = box.label\n",
    "            boxes[box_idx, 1] = ((x1 + x2) / 2)\n",
    "            boxes[box_idx, 2] = ((y1 + y2) / 2)\n",
    "            boxes[box_idx, 3] = (x2 - x1)\n",
    "            boxes[box_idx, 4] = (y2 - y1)\n",
    "\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "class RelativeLabels(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        h, w, _ = img.shape\n",
    "        boxes[:, [1, 3]] /= w\n",
    "        boxes[:, [2, 4]] /= h\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "class AbsoluteLabels(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        h, w, _ = img.shape\n",
    "        boxes[:, [1, 3]] *= w\n",
    "        boxes[:, [2, 4]] *= h\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "class PadSquare(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.PadToAspectRatio(\n",
    "                1.0,\n",
    "                position=\"center-center\").to_deterministic()\n",
    "        ])\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        # Extract image as PyTorch tensor\n",
    "        img = transforms.ToTensor()(img) # (H x W x C) and [0, 255] -> (C x H x W) and [0.0, 1.0]. See: https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html\n",
    "\n",
    "        bb_targets = torch.zeros((len(boxes), 6))\n",
    "        bb_targets[:, 1:] = transforms.ToTensor()(boxes) # tensors are returned without scaling. See: https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html\n",
    "\n",
    "        return img, bb_targets\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        img = F.interpolate(img.unsqueeze(0), size=self.size, mode=\"nearest\").squeeze(0)\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "DEFAULT_TRANSFORMS = transforms.Compose([\n",
    "    AbsoluteLabels(),\n",
    "    PadSquare(),\n",
    "    RelativeLabels(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultAug(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.Sharpen((0.0, 0.1)),\n",
    "            iaa.Affine(rotate=(-0, 0), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
    "            iaa.AddToBrightness((-60, 40)),\n",
    "            iaa.AddToHue((-10, 10)),\n",
    "            iaa.Fliplr(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "class StrongAug(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.Dropout([0.0, 0.01]),\n",
    "            iaa.Sharpen((0.0, 0.1)),\n",
    "            iaa.Affine(rotate=(-10, 10), translate_percent=(-0.1, 0.1), scale=(0.8, 1.5)),\n",
    "            iaa.AddToBrightness((-60, 40)),\n",
    "            iaa.AddToHue((-20, 20)),\n",
    "            iaa.Fliplr(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "AUGMENTATION_TRANSFORMS = transforms.Compose([\n",
    "    AbsoluteLabels(),\n",
    "    DefaultAug(),\n",
    "    PadSquare(),\n",
    "    RelativeLabels(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(img, pad_value):\n",
    "    c, h, w = img.shape\n",
    "    dim_diff = np.abs(h - w)\n",
    "    # (upper / left) padding and (lower / right) padding\n",
    "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "    # Determine padding\n",
    "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
    "    # Add padding\n",
    "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
    "\n",
    "    return img, pad\n",
    "\n",
    "\n",
    "def resize(image, size): # https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
    "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0) # 'nearest' ??? or 'nearest-exact'???\n",
    "    return image\n",
    "\n",
    "\n",
    "class ImageFolder(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_path = self.files[index % len(self.files)]\n",
    "        img = np.array(\n",
    "            Image.open(img_path).convert('RGB'),\n",
    "            dtype=np.uint8)\n",
    "\n",
    "        # Label Placeholder\n",
    "        boxes = np.zeros((1, 5))\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img, _ = self.transform((img, boxes))\n",
    "\n",
    "        return img_path, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, list_path, data_dir, img_size=416, multiscale=True, transform=None):\n",
    "        if isinstance(list_path, list): # Brad\n",
    "            lines = list_path # Brad\n",
    "        else:\n",
    "            with open(list_path, \"r\") as file:\n",
    "                # self.img_files = file.readlines()\n",
    "                lines = file.readlines()\n",
    "\n",
    "        self.img_files = []\n",
    "        self.label_files = []\n",
    "        for path_0 in lines:\n",
    "            if data_dir:\n",
    "                path = os.path.join(data_dir, path_0.strip('/\\\\')) # overwrite path\n",
    "            else:\n",
    "                path = path_0\n",
    "\n",
    "            self.img_files.append(path)\n",
    "\n",
    "            image_dir = os.path.dirname(path)\n",
    "            label_dir = \"labels\".join(image_dir.rsplit(\"images\", 1))\n",
    "            assert label_dir != image_dir, f\"Image path must contain a folder named 'images'! \\n'{image_dir}'\"\n",
    "            label_file = os.path.join(label_dir, os.path.basename(path))\n",
    "            label_file = os.path.splitext(label_file)[0] + '.txt'\n",
    "            self.label_files.append(label_file)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.max_objects = 100\n",
    "        self.multiscale = multiscale # variable size of image\n",
    "        self.min_size = self.img_size - 3 * 32 # applicable only if multiscale = True\n",
    "        self.max_size = self.img_size + 3 * 32 # applicable only if multiscale = True\n",
    "        self.batch_count = 0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # ---------\n",
    "        #  Image\n",
    "        # ---------\n",
    "        try:\n",
    "\n",
    "            img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "            img = np.array(Image.open(img_path).convert('RGB'), dtype=np.uint8)\n",
    "        except Exception:\n",
    "            print(f\"Could not read image '{img_path}'.\")\n",
    "            return\n",
    "\n",
    "        # ---------\n",
    "        #  Label\n",
    "        # ---------\n",
    "        try:\n",
    "            label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "            # Ignore warning if file is empty\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "        except Exception:\n",
    "            print(f\"Could not read label '{label_path}'.\")\n",
    "            return\n",
    "\n",
    "        # -----------\n",
    "        #  Transform\n",
    "        # -----------\n",
    "        if self.transform:\n",
    "            try:\n",
    "                img, bb_targets = self.transform((img, boxes))\n",
    "            except Exception:\n",
    "                print(\"Could not apply transform.\")\n",
    "                return\n",
    "\n",
    "        return img_path, img, bb_targets # boxes (B,5) -> bb_targets (B,6)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        self.batch_count += 1\n",
    "\n",
    "        # Drop invalid images\n",
    "        batch = [data for data in batch if data is not None]\n",
    "\n",
    "        paths, imgs, bb_targets = list(zip(*batch))\n",
    "\n",
    "        # Selects new image size every tenth batch\n",
    "        if self.multiscale and self.batch_count % 10 == 0:\n",
    "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
    "\n",
    "        # Resize images to input shape\n",
    "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(bb_targets):\n",
    "            boxes[:, 0] = i\n",
    "        bb_targets = torch.cat(bb_targets, 0)\n",
    "\n",
    "        return paths, imgs, bb_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     list_path = [r'C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\images\\train2014\\COCO_train2014_000000000009.jpg']\n",
    "#     data_dir = None\n",
    "#     dataset = ListDataset(list_path, data_dir, img_size=416, multiscale=True, transform=AUGMENTATION_TRANSFORMS)\n",
    "\n",
    "# #     img = np.array(Image.open(r'C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\images\\train2014\\COCO_train2014_000000000009.jpg'))\n",
    "# #     print(img.shape) # (480, 640, 3)\n",
    "# #     print(transforms.ToTensor()(img).shape) # torch.Size([3, 480, 640])\n",
    "\n",
    "#     img = np.array(Image.open(dataset.img_files[0]))\n",
    "#     boxes = np.loadtxt(dataset.label_files[0])\n",
    "#     print(f'img.shape = {img.shape}') # img.shape = (480, 640, 3)\n",
    "#     print(f'img.min() = {img.min()}') # img.min() = 0\n",
    "#     print(f'img.max() = {img.max()}') # img.max() = 255\n",
    "#     # print(boxes)\n",
    "\n",
    "#     img2, boxes2 = dataset.transform((img, boxes))\n",
    "#     print(f'img2.shape = {img2.shape}') # img2.shape = torch.Size([3, 640, 640])\n",
    "#     print(f'img2.min() = {img2.min()}') # img2.min() = 0.0\n",
    "#     print(f'img2.max() = {img2.max()}') # img2.max() = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007ListDataset(Dataset):\n",
    "    VOC_BBOX_LABEL_NAMES = [\n",
    "        'aeroplane',\n",
    "        'bicycle',\n",
    "        'bird',\n",
    "        'boat',\n",
    "        'bottle',\n",
    "        'bus',\n",
    "        'car',\n",
    "        'cat',\n",
    "        'chair',\n",
    "        'cow',\n",
    "        'diningtable',\n",
    "        'dog',\n",
    "        'horse',\n",
    "        'motorbike',\n",
    "        'person',\n",
    "        'pottedplant',\n",
    "        'sheep',\n",
    "        'sofa',\n",
    "        'train',\n",
    "        'tvmonitor']\n",
    "\n",
    "    def __init__(self, list_path, data_dir, img_size=416, multiscale=True, transform=None, use_difficult=False):\n",
    "        self.label_names = VOC2007ListDataset.VOC_BBOX_LABEL_NAMES\n",
    "        self.map_i_to_class_name = {i:c for i,c in enumerate(VOC2007ListDataset.VOC_BBOX_LABEL_NAMES)}\n",
    "        self.map_class_name_to_i = {c:i for i,c in enumerate(VOC2007ListDataset.VOC_BBOX_LABEL_NAMES)}\n",
    "        \n",
    "        if isinstance(list_path, list): # Brad\n",
    "            lines = list_path # Brad\n",
    "        else:\n",
    "            with open(list_path, \"r\") as file:\n",
    "                # self.img_files = file.readlines()\n",
    "                lines = file.readlines()\n",
    "\n",
    "        self.img_files = []\n",
    "        self.label_files = []\n",
    "        for filename in lines:\n",
    "            filename = filename.strip()\n",
    "            if len(filename) == 0: # if empty line\n",
    "                continue\n",
    "            self.img_files.append(os.path.join(data_dir, f'JPEGImages/{filename}.jpg'))\n",
    "            self.label_files.append(os.path.join(data_dir, f'Annotations/{filename}.xml'))\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.max_objects = 100\n",
    "        self.multiscale = multiscale # variable size of image\n",
    "        self.min_size = self.img_size - 3 * 32 # applicable only if multiscale = True\n",
    "        self.max_size = self.img_size + 3 * 32 # applicable only if multiscale = True\n",
    "        self.batch_count = 0\n",
    "        self.transform = transform\n",
    "        self.use_difficult = use_difficult\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # ---------\n",
    "        #  Image\n",
    "        # ---------\n",
    "        try:\n",
    "\n",
    "            img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "            img = np.array(Image.open(img_path).convert('RGB'), dtype=np.uint8)\n",
    "        except Exception:\n",
    "            print(f\"Could not read image '{img_path}'.\")\n",
    "            return\n",
    "\n",
    "        # ---------\n",
    "        #  Label\n",
    "        # ---------\n",
    "        try:\n",
    "            label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "\n",
    "            # # Ignore warning if file is empty\n",
    "            # with warnings.catch_warnings():\n",
    "            #     warnings.simplefilter(\"ignore\")\n",
    "            #     boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "\n",
    "            anno = ET.parse(label_path)\n",
    "            bbox = list()\n",
    "            label = list()\n",
    "            difficult = list()\n",
    "            for obj in anno.findall('object'):\n",
    "                # when in not using difficult split, and the object is\n",
    "                # difficult, skipt it.\n",
    "                if not self.use_difficult and int(obj.find('difficult').text) == 1:\n",
    "                    continue\n",
    "\n",
    "                difficult.append(int(obj.find('difficult').text))\n",
    "                bndbox_anno = obj.find('bndbox')\n",
    "                # subtract 1 to make pixel indexes 0-based\n",
    "                bbox.append([\n",
    "                    int(bndbox_anno.find(tag).text) - 1\n",
    "                    # for tag in ('ymin', 'xmin', 'ymax', 'xmax')])\n",
    "                    for tag in ('xmin', 'ymin', 'xmax', 'ymax')])\n",
    "                name = obj.find('name').text.lower().strip()\n",
    "                label.append(self.label_names.index(name))\n",
    "            bbox = np.stack(bbox).astype(np.float32)\n",
    "            label = np.stack(label).astype(np.int32)\n",
    "\n",
    "            boxes = np.concatenate((label.reshape(-1,1), bbox), axis=1)\n",
    "            h, w, _ = img.shape\n",
    "            boxes[:, [1, 3]] /= w # conversion to relative coordinate\n",
    "            boxes[:, [2, 4]] /= h # conversion to relative coordinate\n",
    "            #                         \n",
    "            # print(f'label_path = {label_path}')\n",
    "            # print(f'bbox = {bbox}')\n",
    "            # print(f'label = {label}')\n",
    "            # print(f'boxes = {boxes}')\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"Could not read label '{label_path}'.\")\n",
    "            return\n",
    "\n",
    "        # -----------\n",
    "        #  Transform\n",
    "        # -----------\n",
    "        if self.transform:\n",
    "            try:\n",
    "                img, bb_targets = self.transform((img, boxes))\n",
    "            except Exception:\n",
    "                print(\"Could not apply transform.\")\n",
    "                return\n",
    "\n",
    "        return img_path, img, bb_targets # boxes (B,5) -> bb_targets (B,6)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        self.batch_count += 1\n",
    "\n",
    "        # Drop invalid images\n",
    "        batch = [data for data in batch if data is not None]\n",
    "\n",
    "        paths, imgs, bb_targets = list(zip(*batch))\n",
    "\n",
    "        # Selects new image size every tenth batch\n",
    "        if self.multiscale and self.batch_count % 10 == 0:\n",
    "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
    "\n",
    "        # Resize images to input shape\n",
    "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(bb_targets):\n",
    "            boxes[:, 0] = i\n",
    "        bb_targets = torch.cat(bb_targets, 0)\n",
    "\n",
    "        return paths, imgs, bb_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     list_path = r'C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\voc2007\\VOCdevkit\\VOC2007\\ImageSets\\Main\\trainval.txt'\n",
    "#     data_dir = r'C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\voc2007\\VOCdevkit\\VOC2007'\n",
    "#     dataset = VOC2007ListDataset(list_path, data_dir, img_size=416, multiscale=True, transform=AUGMENTATION_TRANSFORMS)\n",
    "#     print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_model_config(path, debug=False):\n",
    "def parse_model_config(path_or_lines, debug=False): # Brad\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    if isinstance(path_or_lines, list): # Brad\n",
    "        lines = path_or_lines # Brad\n",
    "    else:\n",
    "        # file = open(path, 'r') \n",
    "        file = open(path_or_lines, 'r') # Brad\n",
    "        lines = file.read().split('\\n')\n",
    "    if debug:\n",
    "        print(lines)\n",
    "    lines = [x for x in lines if x and not x.startswith('#')] # exclude an empty or a comment line\n",
    "    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\n",
    "    if debug:\n",
    "        print(lines)\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['):  # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip() # exclude '[' and ']'\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     path = r'C:\\Users\\bomso\\bomsoo1\\python\\GitHub\\object_detector\\config\\yolov3.cfg'\n",
    "#     module_defs = parse_model_config(path, debug=True)\n",
    "#     print(module_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1,2,3'\n",
    "    options['num_workers'] = '10'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, log_dir, log_hist=True):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        if log_hist:    # Check a new folder for each log should be dreated\n",
    "            log_dir = os.path.join(\n",
    "                log_dir,\n",
    "                datetime.datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\"))\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def list_of_scalars_summary(self, tag_value_pairs, step):\n",
    "        \"\"\"Log scalar variables.\"\"\"\n",
    "        for tag, value in tag_value_pairs:\n",
    "            self.writer.add_scalar(tag, value, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/config/yolov3.cfg\n",
    "CONFIG_LINES_FOR_YOLO_V3 = \"\"\"\n",
    "[net]\n",
    "# Testing\n",
    "#batch=1\n",
    "#subdivisions=1\n",
    "# Training\n",
    "batch=16\n",
    "subdivisions=1\n",
    "width=416\n",
    "height=416\n",
    "channels=3\n",
    "momentum=0.9\n",
    "decay=0.0005\n",
    "angle=0\n",
    "saturation = 1.5\n",
    "exposure = 1.5\n",
    "hue=.1\n",
    "\n",
    "learning_rate=0.0001\n",
    "burn_in=1000\n",
    "max_batches = 500200\n",
    "policy=steps\n",
    "steps=400000,450000\n",
    "scales=.1,.1\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "# Downsample\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=1024\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[shortcut]\n",
    "from=-3\n",
    "activation=linear\n",
    "\n",
    "######################\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=512\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=1024\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=255\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 6,7,8\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "classes=80\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\n",
    "\n",
    "[route]\n",
    "layers = -4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = -1, 61\n",
    "\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=256\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=512\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=255\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 3,4,5\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "classes=80\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\n",
    "\n",
    "\n",
    "[route]\n",
    "layers = -4\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[upsample]\n",
    "stride=2\n",
    "\n",
    "[route]\n",
    "layers = -1, 36\n",
    "\n",
    "\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=128\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "filters=256\n",
    "activation=leaky\n",
    "\n",
    "[convolutional]\n",
    "size=1\n",
    "stride=1\n",
    "pad=1\n",
    "filters=255\n",
    "activation=linear\n",
    "\n",
    "\n",
    "[yolo]\n",
    "mask = 0,1,2\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "classes=80\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\"\"\".split('\\n')\n",
    "\n",
    "# print(CONFIG_LINES_FOR_YOLO_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modules(module_defs: List[dict]) -> Tuple[dict, nn.ModuleList]:\n",
    "    \"\"\"\n",
    "    Constructs module list of layer blocks from module configuration in module_defs\n",
    "\n",
    "    :param module_defs: List of dictionaries with module definitions\n",
    "    :return: Hyperparameters and pytorch module list\n",
    "    \"\"\"\n",
    "    hyperparams = module_defs.pop(0)\n",
    "    hyperparams.update({\n",
    "        'batch': int(hyperparams['batch']),\n",
    "        'subdivisions': int(hyperparams['subdivisions']),\n",
    "        'width': int(hyperparams['width']),\n",
    "        'height': int(hyperparams['height']),\n",
    "        'channels': int(hyperparams['channels']),\n",
    "        'optimizer': hyperparams.get('optimizer'),\n",
    "        'momentum': float(hyperparams['momentum']),\n",
    "        'decay': float(hyperparams['decay']),\n",
    "        'learning_rate': float(hyperparams['learning_rate']),\n",
    "        'burn_in': int(hyperparams['burn_in']),\n",
    "        'max_batches': int(hyperparams['max_batches']),\n",
    "        'policy': hyperparams['policy'],\n",
    "        'lr_steps': list(zip(map(int,   hyperparams[\"steps\"].split(\",\")),\n",
    "                             map(float, hyperparams[\"scales\"].split(\",\"))))\n",
    "    })\n",
    "    assert hyperparams[\"height\"] == hyperparams[\"width\"], \\\n",
    "        \"Height and width should be equal! Non square images are padded with zeros.\"\n",
    "    output_filters = [hyperparams[\"channels\"]]\n",
    "    module_list = nn.ModuleList()\n",
    "    for module_i, module_def in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if module_def[\"type\"] == \"convolutional\":\n",
    "            bn = int(module_def[\"batch_normalize\"])\n",
    "            filters = int(module_def[\"filters\"])\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            pad = (kernel_size - 1) // 2\n",
    "            modules.add_module(\n",
    "                f\"conv_{module_i}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=output_filters[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=int(module_def[\"stride\"]),\n",
    "                    padding=pad,\n",
    "                    bias=not bn,\n",
    "                ),\n",
    "            )\n",
    "            if bn:\n",
    "                modules.add_module(f\"batch_norm_{module_i}\",\n",
    "                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))\n",
    "            if module_def[\"activation\"] == \"leaky\":\n",
    "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
    "            elif module_def[\"activation\"] == \"mish\":\n",
    "                modules.add_module(f\"mish_{module_i}\", nn.Mish())\n",
    "            elif module_def[\"activation\"] == \"logistic\":\n",
    "                modules.add_module(f\"sigmoid_{module_i}\", nn.Sigmoid())\n",
    "            elif module_def[\"activation\"] == \"swish\":\n",
    "                modules.add_module(f\"swish_{module_i}\", nn.SiLU())\n",
    "\n",
    "        elif module_def[\"type\"] == \"maxpool\":\n",
    "            kernel_size = int(module_def[\"size\"])\n",
    "            stride = int(module_def[\"stride\"])\n",
    "            if kernel_size == 2 and stride == 1:\n",
    "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride,\n",
    "                                   padding=int((kernel_size - 1) // 2))\n",
    "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
    "\n",
    "        elif module_def[\"type\"] == \"upsample\":\n",
    "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
    "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
    "\n",
    "        elif module_def[\"type\"] == \"route\":\n",
    "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
    "            filters = sum([output_filters[1:][i] for i in layers]) // int(module_def.get(\"groups\", 1))\n",
    "            modules.add_module(f\"route_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"shortcut\":\n",
    "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
    "            modules.add_module(f\"shortcut_{module_i}\", nn.Sequential())\n",
    "\n",
    "        elif module_def[\"type\"] == \"yolo\":\n",
    "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
    "            # Extract anchors\n",
    "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
    "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "            anchors = [anchors[i] for i in anchor_idxs]\n",
    "            num_classes = int(module_def[\"classes\"])\n",
    "            new_coords = bool(module_def.get(\"new_coords\", False))\n",
    "            # Define detection layer\n",
    "            yolo_layer = YOLOLayer(anchors, num_classes, new_coords)\n",
    "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    return hyperparams, module_list\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor, mode: str = \"nearest\"):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
    "        return x\n",
    "\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    \"\"\"Detection layer\"\"\"\n",
    "\n",
    "    def __init__(self, anchors: List[Tuple[int, int]], num_classes: int, new_coords: bool):\n",
    "        \"\"\"\n",
    "        Create a YOLO layer\n",
    "\n",
    "        :param anchors: List of anchors\n",
    "        :param num_classes: Number of classes\n",
    "        :param new_coords: Whether to use the new coordinate format from YOLO V7\n",
    "        \"\"\"\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.new_coords = new_coords\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.no = num_classes + 5  # number of outputs per anchor\n",
    "        self.grid = torch.zeros(1)  # TODO\n",
    "\n",
    "        anchors = torch.tensor(list(chain(*anchors))).float().view(-1, 2)\n",
    "        self.register_buffer('anchors', anchors)\n",
    "        self.register_buffer(\n",
    "            'anchor_grid', anchors.clone().view(1, -1, 1, 1, 2))\n",
    "        self.stride = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the YOLO layer\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :param img_size: Size of the input image\n",
    "        \"\"\"\n",
    "        stride = img_size // x.size(2)\n",
    "        self.stride = stride\n",
    "        bs, _, ny, nx = x.shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
    "        x = x.view(bs, self.num_anchors, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "        if not self.training:  # inference\n",
    "            if self.grid.shape[2:4] != x.shape[2:4]:\n",
    "                self.grid = self._make_grid(nx, ny).to(x.device)\n",
    "\n",
    "            if self.new_coords:\n",
    "                x[..., 0:2] = (x[..., 0:2] + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = x[..., 2:4] ** 2 * (4 * self.anchor_grid) # wh\n",
    "            else:\n",
    "                x[..., 0:2] = (x[..., 0:2].sigmoid() + self.grid) * stride  # xy\n",
    "                x[..., 2:4] = torch.exp(x[..., 2:4]) * self.anchor_grid # wh\n",
    "                x[..., 4:] = x[..., 4:].sigmoid() # conf, cls\n",
    "            x = x.view(bs, -1, self.no)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_grid(nx: int = 20, ny: int = 20) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a grid of (x, y) coordinates\n",
    "\n",
    "        :param nx: Number of x coordinates\n",
    "        :param ny: Number of y coordinates\n",
    "        \"\"\"\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')\n",
    "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    \"\"\"YOLOv3 object detection model\"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        super(Darknet, self).__init__()\n",
    "        if config_path is not None: # Brad\n",
    "            self.module_defs = parse_model_config(config_path)\n",
    "            print(f'> model configuration is successfully loaded from: {config_path}')\n",
    "        else:\n",
    "            self.module_defs = parse_model_config(CONFIG_LINES_FOR_YOLO_V3) # Brad\n",
    "            print(f'> No model config file is specified. So, the default configuration for yolo v3 is applied. check \"CONFIG_LINES_FOR_YOLO_V3\" inside the code.')\n",
    "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
    "        self.yolo_layers = [layer[0] for layer in self.module_list if isinstance(layer[0], YOLOLayer)]\n",
    "        self.seen = 0\n",
    "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_size = x.size(2)\n",
    "        layer_outputs, yolo_outputs = [], []\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = module(x)\n",
    "            elif module_def[\"type\"] == \"route\":\n",
    "                combined_outputs = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
    "                group_size = combined_outputs.shape[1] // int(module_def.get(\"groups\", 1))\n",
    "                group_id = int(module_def.get(\"group_id\", 0))\n",
    "                x = combined_outputs[:, group_size * group_id : group_size * (group_id + 1)] # Slice groupings used by yolo v4\n",
    "            elif module_def[\"type\"] == \"shortcut\":\n",
    "                layer_i = int(module_def[\"from\"])\n",
    "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            elif module_def[\"type\"] == \"yolo\":\n",
    "                x = module[0](x, img_size)\n",
    "                yolo_outputs.append(x)\n",
    "            layer_outputs.append(x)\n",
    "        return yolo_outputs if self.training else torch.cat(yolo_outputs, 1)\n",
    "\n",
    "    def load_darknet_weights(self, weights_path):\n",
    "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
    "\n",
    "        # Open the weights file\n",
    "        with open(weights_path, \"rb\") as f:\n",
    "            # First five are header values\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)\n",
    "            self.header_info = header  # Needed to write header when saving weights\n",
    "            self.seen = header[3]  # number of images seen during training\n",
    "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
    "\n",
    "        # Establish cutoff for loading backbone weights\n",
    "        cutoff = None\n",
    "        # If the weights file has a cutoff, we can find out about it by looking at the filename\n",
    "        # examples: darknet53.conv.74 -> cutoff is 74\n",
    "        filename = os.path.basename(weights_path)\n",
    "        if \".conv.\" in filename:\n",
    "            try:\n",
    "                cutoff = int(filename.split(\".\")[-1])  # use last part of filename\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        ptr = 0\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    # Load BN bias, weights, running mean and running variance\n",
    "                    bn_layer = module[1]\n",
    "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
    "                    # Bias\n",
    "                    bn_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_b)\n",
    "                    ptr += num_b\n",
    "                    # Weight\n",
    "                    bn_w = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_w)\n",
    "                    ptr += num_b\n",
    "                    # Running Mean\n",
    "                    bn_rm = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
    "                    ptr += num_b\n",
    "                    # Running Var\n",
    "                    bn_rv = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_rv)\n",
    "                    ptr += num_b\n",
    "                else:\n",
    "                    # Load conv. bias\n",
    "                    num_b = conv_layer.bias.numel()\n",
    "                    conv_b = torch.from_numpy(\n",
    "                        weights[ptr: ptr + num_b]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_b)\n",
    "                    ptr += num_b\n",
    "                # Load conv. weights\n",
    "                num_w = conv_layer.weight.numel()\n",
    "                conv_w = torch.from_numpy(\n",
    "                    weights[ptr: ptr + num_w]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_w)\n",
    "                ptr += num_w\n",
    "\n",
    "    def save_darknet_weights(self, path, cutoff=-1):\n",
    "        \"\"\"\n",
    "            @:param path    - path of the new weights file\n",
    "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
    "        \"\"\"\n",
    "        fp = open(path, \"wb\")\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(fp)\n",
    "\n",
    "        # Iterate through layers\n",
    "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "            if module_def[\"type\"] == \"convolutional\":\n",
    "                conv_layer = module[0]\n",
    "                # If batch norm, load bn first\n",
    "                if module_def[\"batch_normalize\"]:\n",
    "                    bn_layer = module[1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv bias\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
    "                # Load conv weights\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "\n",
    "def load_model(model_path, weights_path=None):\n",
    "    \"\"\"Loads the yolo model from file.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :return: Returns model\n",
    "    :rtype: Darknet\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Select device for inference\n",
    "    model = Darknet(model_path).to(device)\n",
    "\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    # If pretrained weights are specified, start from checkpoint or weight file\n",
    "    if weights_path:\n",
    "        if weights_path.endswith(\".pth\"):\n",
    "            # Load checkpoint weights\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "        else:\n",
    "            # Load darknet weights\n",
    "            model.load_darknet_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_file(model_path, data_dir, weights_path, img_path, class_names, batch_size=8, img_size=416,\n",
    "                        n_cpu=8, iou_thres=0.5, conf_thres=0.5, nms_thres=0.5, dataset_name='COCO2014', verbose=True):\n",
    "    \"\"\"Evaluate model on validation dataset.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param class_names: List of class names\n",
    "    :type class_names: [str]\n",
    "    :param batch_size: Size of each image batch, defaults to 8\n",
    "    :type batch_size: int, optional\n",
    "    :param img_size: Size of each image dimension for yolo, defaults to 416\n",
    "    :type img_size: int, optional\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation, defaults to 8\n",
    "    :type n_cpu: int, optional\n",
    "    :param iou_thres: IOU threshold required to qualify as detected, defaults to 0.5\n",
    "    :type iou_thres: float, optional\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    :param verbose: If True, prints stats of model, defaults to True\n",
    "    :type verbose: bool, optional\n",
    "    :return: Returns precision, recall, AP, f1, ap_class\n",
    "    \"\"\"\n",
    "    dataloader = _create_validation_data_loader(img_path, data_dir, batch_size, img_size, n_cpu, dataset_name=dataset_name)\n",
    "    model = load_model(model_path, weights_path)\n",
    "    metrics_output = _evaluate(\n",
    "        model,\n",
    "        dataloader,\n",
    "        class_names,\n",
    "        img_size,\n",
    "        iou_thres,\n",
    "        conf_thres,\n",
    "        nms_thres,\n",
    "        verbose)\n",
    "    return metrics_output\n",
    "\n",
    "\n",
    "def print_eval_stats(metrics_output, class_names, verbose):\n",
    "    if metrics_output is not None:\n",
    "        precision, recall, AP, f1, ap_class = metrics_output\n",
    "        if verbose:\n",
    "            # Prints class AP and mean AP\n",
    "            ap_table = [[\"Index\", \"Class\", \"AP\"]]\n",
    "            for i, c in enumerate(ap_class):\n",
    "                ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "            print(AsciiTable(ap_table).table)\n",
    "        print(f\"---- mAP {AP.mean():.5f} ----\")\n",
    "    else:\n",
    "        print(\"---- mAP not measured (no detections found by model) ----\")\n",
    "\n",
    "\n",
    "def _evaluate(model, dataloader, class_names, img_size, iou_thres, conf_thres, nms_thres, verbose):\n",
    "    \"\"\"Evaluate model on validation dataset.\n",
    "\n",
    "    :param model: Model to evaluate\n",
    "    :type model: models.Darknet\n",
    "    :param dataloader: Dataloader provides the batches of images with targets\n",
    "    :type dataloader: DataLoader\n",
    "    :param class_names: List of class names\n",
    "    :type class_names: [str]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param iou_thres: IOU threshold required to qualify as detected\n",
    "    :type iou_thres: float\n",
    "    :param conf_thres: Object confidence threshold\n",
    "    :type conf_thres: float\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression\n",
    "    :type nms_thres: float\n",
    "    :param verbose: If True, prints stats of model\n",
    "    :type verbose: bool\n",
    "    :return: Returns precision, recall, AP, f1, ap_class\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
    "    for _, imgs, targets in tqdm.tqdm(dataloader, desc=\"Validating\"):\n",
    "        # Extract labels\n",
    "        labels += targets[:, 1].tolist()\n",
    "        # Rescale target\n",
    "        targets[:, 2:] = xywh2xyxy(targets[:, 2:])\n",
    "        targets[:, 2:] *= img_size\n",
    "\n",
    "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(imgs)\n",
    "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, iou_thres=nms_thres)\n",
    "\n",
    "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
    "\n",
    "    if len(sample_metrics) == 0:  # No detections over whole validation set.\n",
    "        print(\"---- No detections over whole validation set ----\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate sample statistics\n",
    "    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "    metrics_output = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "\n",
    "    print_eval_stats(metrics_output, class_names, verbose)\n",
    "\n",
    "    return metrics_output\n",
    "\n",
    "\n",
    "def _create_validation_data_loader(img_path, data_dir, batch_size, img_size, n_cpu, dataset_name='COCO2014'):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for validation.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    if dataset_name =='COCO2014':\n",
    "        dataset = ListDataset(img_path, data_dir, img_size=img_size, multiscale=False, transform=DEFAULT_TRANSFORMS)\n",
    "    elif dataset_name =='VOC2007':\n",
    "        dataset = VOC2007ListDataset(img_path, data_dir, img_size=img_size, multiscale=False, transform=DEFAULT_TRANSFORMS, use_difficult=True)\n",
    "    else:\n",
    "        raise(Exception(f'Brad error: no such dataset name: \"{dataset_name}\" ...'))\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=n_cpu, pin_memory=True, collate_fn=dataset.collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def run_test(command_args):\n",
    "    print_environment_info()\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate validation data.\")\n",
    "    # parser.add_argument(\"-m\", \"--model\", type=str, default=\"config/yolov3.cfg\", help=\"Path to model definition file (.cfg)\")\n",
    "    parser.add_argument(\"-m\", \"--model\", type=str, help=\"Path to model definition file (.cfg)\") # Brad\n",
    "    parser.add_argument(\"-w\", \"--weights\", type=str, default=\"weights/yolov3.weights\", help=\"Path to weights or checkpoint file (.weights or .pth)\")\n",
    "    # parser.add_argument(\"-d\", \"--data\", type=str, default=\"config/coco.data\", help=\"Path to data config file (.data)\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"COCO2014\", help=\"COCO2014, VOC2007, etc.\") # Brad\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"data/coco\", help=\"Path to the root directory where 'images' and 'annotations' sub-folders exist\") # Brad\n",
    "    parser.add_argument(\"--valid\", type=str, default=\"data/coco/5k.part\", help=\"Path to the file containg the list of validation image files\") # Brad\n",
    "    # parser.add_argument(\"--names\", type=str, default=\"data/coco.names\", help=\"Path to the file containg the list of all the class names\") # Brad\n",
    "    parser.add_argument(\"--names\", type=str, help=\"Path to the file containg the list of all the class names\") # Brad\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=8, help=\"Size of each image batch\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action='store_true', help=\"Makes the validation more verbose\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=416, help=\"Size of each image dimension for yolo\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"Number of cpu threads to use during batch generation\")\n",
    "    parser.add_argument(\"--iou_thres\", type=float, default=0.5, help=\"IOU threshold required to qualify as detected\")\n",
    "    parser.add_argument(\"--conf_thres\", type=float, default=0.01, help=\"Object confidence threshold\")\n",
    "    parser.add_argument(\"--nms_thres\", type=float, default=0.4, help=\"IOU threshold for non-maximum suppression\")\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(command_args) # Brad: for jupyter notebook testing    \n",
    "    print(f\"Command line arguments: {args}\")\n",
    "\n",
    "    valid_path = args.valid # Path to file containing all images for validation\n",
    "    if args.dataset_name =='VOC2007':\n",
    "        class_names = VOC2007ListDataset.VOC_BBOX_LABEL_NAMES\n",
    "    else:\n",
    "        class_names = load_classes(args.names)  # List of class names\n",
    "\n",
    "    precision, recall, AP, f1, ap_class = evaluate_model_file(\n",
    "        args.model, args.data_dir, \n",
    "        args.weights,\n",
    "        valid_path,\n",
    "        class_names,\n",
    "        batch_size=args.batch_size,\n",
    "        img_size=args.img_size,\n",
    "        n_cpu=args.n_cpu,\n",
    "        iou_thres=args.iou_thres,\n",
    "        conf_thres=args.conf_thres,\n",
    "        nms_thres=args.nms_thres,\n",
    "        dataset_name=args.dataset_name,\n",
    "        verbose=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_directory(model_path, weights_path, img_path, classes, output_path,\n",
    "                     batch_size=8, img_size=416, n_cpu=8, conf_thres=0.5, nms_thres=0.5):\n",
    "    \"\"\"Detects objects on all images in specified directory and saves output images with drawn detections.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :param img_path: Path to directory with images to inference\n",
    "    :type img_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    :param output_path: Path to output directory\n",
    "    :type output_path: str\n",
    "    :param batch_size: Size of each image batch, defaults to 8\n",
    "    :type batch_size: int, optional\n",
    "    :param img_size: Size of each image dimension for yolo, defaults to 416\n",
    "    :type img_size: int, optional\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation, defaults to 8\n",
    "    :type n_cpu: int, optional\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    \"\"\"\n",
    "    dataloader = _create_data_loader(img_path, batch_size, img_size, n_cpu)\n",
    "    model = load_model(model_path, weights_path)\n",
    "    img_detections, imgs = detect(\n",
    "        model,\n",
    "        dataloader,\n",
    "        output_path,\n",
    "        conf_thres,\n",
    "        nms_thres)\n",
    "    _draw_and_save_output_images(\n",
    "        img_detections, imgs, img_size, output_path, classes)\n",
    "\n",
    "    print(f\"---- Detections were saved to: '{output_path}' ----\")\n",
    "\n",
    "\n",
    "def detect_image(model, image, img_size=416, conf_thres=0.5, nms_thres=0.5):\n",
    "    \"\"\"Inferences one image with model.\n",
    "\n",
    "    :param model: Model for inference\n",
    "    :type model: models.Darknet\n",
    "    :param image: Image to inference\n",
    "    :type image: nd.array\n",
    "    :param img_size: Size of each image dimension for yolo, defaults to 416\n",
    "    :type img_size: int, optional\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    :return: Detections on image with each detection in the format: [x1, y1, x2, y2, confidence, class]\n",
    "    :rtype: nd.array\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Configure input\n",
    "    input_img = transforms.Compose([\n",
    "        DEFAULT_TRANSFORMS,\n",
    "        Resize(img_size)])(\n",
    "            (image, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        input_img = input_img.to(\"cuda\")\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "        detections = rescale_boxes(detections[0], img_size, image.shape[:2])\n",
    "    return detections.numpy()\n",
    "\n",
    "\n",
    "def detect(model, dataloader, output_path, conf_thres, nms_thres):\n",
    "    \"\"\"Inferences images with model.\n",
    "\n",
    "    :param model: Model for inference\n",
    "    :type model: models.Darknet\n",
    "    :param dataloader: Dataloader provides the batches of images to inference\n",
    "    :type dataloader: DataLoader\n",
    "    :param output_path: Path to output directory\n",
    "    :type output_path: str\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    :return: List of detections. The coordinates are given for the padded image that is provided by the dataloader.\n",
    "        Use `utils.rescale_boxes` to transform them into the desired input image coordinate system before its transformed by the dataloader),\n",
    "        List of input image paths\n",
    "    :rtype: [Tensor], [str]\n",
    "    \"\"\"\n",
    "    # Create output directory, if missing\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    img_detections = []  # Stores detections for each image index\n",
    "    imgs = []  # Stores image paths\n",
    "\n",
    "    for (img_paths, input_imgs) in tqdm.tqdm(dataloader, desc=\"Detecting\"):\n",
    "        # Configure input\n",
    "        input_imgs = Variable(input_imgs.type(Tensor))\n",
    "\n",
    "        # Get detections\n",
    "        with torch.no_grad():\n",
    "            detections = model(input_imgs)\n",
    "            detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "\n",
    "        # Store image and detections\n",
    "        img_detections.extend(detections)\n",
    "        imgs.extend(img_paths)\n",
    "    return img_detections, imgs\n",
    "\n",
    "\n",
    "def _draw_and_save_output_images(img_detections, imgs, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output images and stores them.\n",
    "\n",
    "    :param img_detections: List of detections\n",
    "    :type img_detections: [Tensor]\n",
    "    :param imgs: List of paths to image files\n",
    "    :type imgs: [str]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through images and save plot of detections\n",
    "    for (image_path, detections) in zip(imgs, img_detections):\n",
    "        print(f\"Image {image_path}:\")\n",
    "        _draw_and_save_output_image(\n",
    "            image_path, detections, img_size, output_path, classes)\n",
    "\n",
    "\n",
    "def _draw_and_save_output_image(image_path, detections, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output image and stores this.\n",
    "\n",
    "    :param image_path: Path to input image\n",
    "    :type image_path: str\n",
    "    :param detections: List of detections on image\n",
    "    :type detections: [Tensor]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(image_path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    # Rescale boxes to original image\n",
    "    detections = rescale_boxes(detections, img_size, img.shape[:2])\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    # Bounding-box colors\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, n_cls_preds)]\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_pred in detections:\n",
    "\n",
    "        print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n",
    "\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        # Create a Rectangle patch\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "        # Add the bbox to the plot\n",
    "        ax.add_patch(bbox)\n",
    "        # Add label\n",
    "        plt.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            s=f\"{classes[int(cls_pred)]}: {conf:.2f}\",\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": color, \"pad\": 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis(\"off\")\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    filename = os.path.basename(image_path).split(\".\")[0]\n",
    "    output_path = os.path.join(output_path, f\"{filename}.png\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu):\n",
    "    \"\"\"Creates a DataLoader for inferencing.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ImageFolder(\n",
    "        img_path,\n",
    "        transform=transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)]))\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_cpu,\n",
    "        pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def run_detect():\n",
    "    print_environment_info()\n",
    "    parser = argparse.ArgumentParser(description=\"Detect objects on images.\")\n",
    "    parser.add_argument(\"-m\", \"--model\", type=str, default=\"config/yolov3.cfg\", help=\"Path to model definition file (.cfg)\")\n",
    "    parser.add_argument(\"-w\", \"--weights\", type=str, default=\"weights/yolov3.weights\", help=\"Path to weights or checkpoint file (.weights or .pth)\")\n",
    "    parser.add_argument(\"-i\", \"--images\", type=str, default=\"data/samples\", help=\"Path to directory with images to inference\")\n",
    "    parser.add_argument(\"-c\", \"--classes\", type=str, default=\"data/coco.names\", help=\"Path to classes label file (.names)\")\n",
    "    parser.add_argument(\"-o\", \"--output\", type=str, default=\"output\", help=\"Path to output directory\")\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1, help=\"Size of each image batch\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=416, help=\"Size of each image dimension for yolo\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"Number of cpu threads to use during batch generation\")\n",
    "    parser.add_argument(\"--conf_thres\", type=float, default=0.5, help=\"Object confidence threshold\")\n",
    "    parser.add_argument(\"--nms_thres\", type=float, default=0.4, help=\"IOU threshold for non-maximum suppression\")\n",
    "    args = parser.parse_args()\n",
    "    print(f\"Command line arguments: {args}\")\n",
    "\n",
    "    # Extract class names from file\n",
    "    classes = load_classes(args.classes)  # List of class names\n",
    "\n",
    "    detect_directory(\n",
    "        args.model,\n",
    "        args.weights,\n",
    "        args.images,\n",
    "        classes,\n",
    "        args.output,\n",
    "        batch_size=args.batch_size,\n",
    "        img_size=args.img_size,\n",
    "        n_cpu=args.n_cpu,\n",
    "        conf_thres=args.conf_thres,\n",
    "        nms_thres=args.nms_thres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_data_loader(img_path, data_dir, batch_size, img_size, n_cpu, multiscale_training=False, dataset_name='COCO2014'):\n",
    "    \"\"\"Creates a DataLoader for training.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to training images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :param multiscale_training: Scale images to different sizes randomly\n",
    "    :type multiscale_training: bool\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    if dataset_name =='COCO2014':\n",
    "        dataset = ListDataset(img_path, data_dir, img_size=img_size, multiscale=multiscale_training, transform=AUGMENTATION_TRANSFORMS)\n",
    "    elif dataset_name =='VOC2007':\n",
    "        dataset = VOC2007ListDataset(img_path, data_dir, img_size=img_size, multiscale=multiscale_training, transform=AUGMENTATION_TRANSFORMS, use_difficult=False)\n",
    "    else:\n",
    "        raise(Exception(f'Brad error: no such dataset name: \"{dataset_name}\" ...'))\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=n_cpu, pin_memory=True, collate_fn=dataset.collate_fn, worker_init_fn=worker_seed_set)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def run_train(command_args, history={}):\n",
    "    print_environment_info()\n",
    "    parser = argparse.ArgumentParser(description=\"Trains the YOLO model.\")\n",
    "    # parser.add_argument(\"-m\", \"--model\", type=str, default=\"config/yolov3.cfg\", help=\"Path to model definition file (.cfg)\")\n",
    "    parser.add_argument(\"-m\", \"--model\", type=str, help=\"Path to model definition file (.cfg)\") # Brad\n",
    "    # parser.add_argument(\"-d\", \"--data\", type=str, default=\"config/coco.data\", help=\"Path to data config file (.data)\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"COCO2014\", help=\"COCO2014, VOC2007, etc.\") # Brad\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"data/coco\", help=\"Path to the root directory where 'images' and 'annotations' sub-folders exist\") # Brad\n",
    "    parser.add_argument(\"--train\", type=str, default=\"data/coco/trainvalno5k.part\", help=\"Path to the file containg the list of training image files\") # Brad\n",
    "    parser.add_argument(\"--valid\", type=str, default=\"data/coco/5k.part\", help=\"Path to the file containg the list of validation image files\") # Brad\n",
    "    # parser.add_argument(\"--names\", type=str, default=\"data/coco.names\", help=\"Path to the file containg the list of all the class names\") # Brad\n",
    "    parser.add_argument(\"--names\", type=str, help=\"Path to the file containg the list of all the class names\") # Brad\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=300, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action='store_true', help=\"Makes the training more verbose\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"Number of cpu threads to use during batch generation\")\n",
    "    parser.add_argument(\"--pretrained_weights\", type=str, help=\"Path to checkpoint file (.weights or .pth). Starts training from checkpoint model\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"Interval of epochs between saving model weights\")\n",
    "    parser.add_argument(\"--evaluation_interval\", type=int, default=1, help=\"Interval of epochs between evaluations on validation set\")\n",
    "    parser.add_argument(\"--multiscale_training\", action=\"store_true\", help=\"Allow multi-scale training\")\n",
    "    parser.add_argument(\"--iou_thres\", type=float, default=0.5, help=\"Evaluation: IOU threshold required to qualify as detected\")\n",
    "    parser.add_argument(\"--conf_thres\", type=float, default=0.1, help=\"Evaluation: Object confidence threshold\")\n",
    "    parser.add_argument(\"--nms_thres\", type=float, default=0.5, help=\"Evaluation: IOU threshold for non-maximum suppression\")\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"logs\", help=\"Directory for training log files (e.g. for TensorBoard)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=-1, help=\"Makes results reproducable. Set -1 to disable.\")\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(command_args) # Brad: for jupyter notebook testing\n",
    "    print(f\"Command line arguments: {args}\")\n",
    "\n",
    "    if args.seed != -1:\n",
    "        provide_determinism(args.seed)\n",
    "\n",
    "    logger = Logger(args.logdir)  # Tensorboard logger\n",
    "\n",
    "    # Create output directories if missing\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    # Get data configuration\n",
    "    # data_config = parse_data_config(args.data)\n",
    "    # train_path = data_config[\"train\"]\n",
    "    # valid_path = data_config[\"valid\"]\n",
    "    # class_names = load_classes(data_config[\"names\"])\n",
    "    train_path = args.train # Brad\n",
    "    valid_path = args.valid # Brad\n",
    "    if args.dataset_name =='VOC2007':\n",
    "        class_names = VOC2007ListDataset.VOC_BBOX_LABEL_NAMES\n",
    "    else:\n",
    "        class_names = load_classes(args.names) # Brad\n",
    "    print(f'>> len(class_names) = {len(class_names)}')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'>> device = {device}')\n",
    "\n",
    "    # ############\n",
    "    # Create model\n",
    "    # ############\n",
    "\n",
    "    print(f'>> pretrained_weights = {args.pretrained_weights}')\n",
    "\n",
    "    model = load_model(args.model, args.pretrained_weights)\n",
    "\n",
    "    # # Print model\n",
    "    # if args.verbose:\n",
    "    #     summary(model, input_size=(3, model.hyperparams['height'], model.hyperparams['height']))\n",
    "\n",
    "    mini_batch_size = model.hyperparams['batch'] // model.hyperparams['subdivisions']\n",
    "\n",
    "    # #################\n",
    "    # Create Dataloader\n",
    "    # #################\n",
    "    data_dir = args.data_dir\n",
    "\n",
    "    # Load training dataloader\n",
    "    dataloader = _create_data_loader(\n",
    "        train_path, data_dir,\n",
    "        mini_batch_size,\n",
    "        model.hyperparams['height'],\n",
    "        args.n_cpu,\n",
    "        args.multiscale_training, dataset_name=args.dataset_name)\n",
    "\n",
    "    # Load validation dataloader\n",
    "    validation_dataloader = _create_validation_data_loader(\n",
    "        valid_path, data_dir,\n",
    "        mini_batch_size,\n",
    "        model.hyperparams['height'],\n",
    "        args.n_cpu, dataset_name=args.dataset_name)\n",
    "\n",
    "    # ################\n",
    "    # Create optimizer\n",
    "    # ################\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    if (model.hyperparams['optimizer'] in [None, \"adam\"]):\n",
    "        optimizer = optim.Adam(\n",
    "            params,\n",
    "            lr=model.hyperparams['learning_rate'],\n",
    "            weight_decay=model.hyperparams['decay'],\n",
    "        )\n",
    "    elif (model.hyperparams['optimizer'] == \"sgd\"):\n",
    "        optimizer = optim.SGD(\n",
    "            params,\n",
    "            lr=model.hyperparams['learning_rate'],\n",
    "            weight_decay=model.hyperparams['decay'],\n",
    "            momentum=model.hyperparams['momentum'])\n",
    "    else:\n",
    "        print(\"Unknown optimizer. Please choose between (adam, sgd).\")\n",
    "\n",
    "    # skip epoch zero, because then the calculations for when to evaluate/checkpoint makes more intuitive sense\n",
    "    # e.g. when you stop after 30 epochs and evaluate every 10 epochs then the evaluations happen after: 10,20,30\n",
    "    # instead of: 0, 10, 20\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "\n",
    "        print(\"\\n---- Training Model ----\")\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=f\"Training Epoch {epoch}\")):\n",
    "            batches_done = len(dataloader) * epoch + batch_i\n",
    "\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(imgs) # outputs: [(B, C, H//32, W//32, 1 + 4 num_class), (B, C, H//16, W//16, 1 + 4 num_class), (B, C, H//8, W//8, 1 + 4 num_class)]\n",
    "\n",
    "            loss, loss_components = compute_loss(outputs, targets, model)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ###############\n",
    "            # Run optimizer\n",
    "            ###############\n",
    "\n",
    "            if batches_done % model.hyperparams['subdivisions'] == 0:\n",
    "                # Adapt learning rate\n",
    "                # Get learning rate defined in cfg\n",
    "                lr = model.hyperparams['learning_rate']\n",
    "                if batches_done < model.hyperparams['burn_in']:\n",
    "                    # Burn in\n",
    "                    lr *= (batches_done / model.hyperparams['burn_in'])\n",
    "                else:\n",
    "                    # Set and parse the learning rate to the steps defined in the cfg\n",
    "                    for threshold, value in model.hyperparams['lr_steps']:\n",
    "                        if batches_done > threshold:\n",
    "                            lr *= value\n",
    "                # Log the learning rate\n",
    "                logger.scalar_summary(\"train/learning_rate\", lr, batches_done)\n",
    "                # Set learning rate\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "                # Run optimizer\n",
    "                optimizer.step()\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # ############\n",
    "            # Log progress\n",
    "            # ############\n",
    "            if args.verbose:\n",
    "                print(AsciiTable(\n",
    "                    [\n",
    "                        [\"Type\", \"Value\"],\n",
    "                        [\"IoU loss\", float(loss_components[0])],\n",
    "                        [\"Object loss\", float(loss_components[1])],\n",
    "                        [\"Class loss\", float(loss_components[2])],\n",
    "                        [\"Loss\", float(loss_components[3])],\n",
    "                        [\"Batch loss\", to_cpu(loss).item()],\n",
    "                    ]).table)\n",
    "\n",
    "            # Tensorboard logging\n",
    "            tensorboard_log = [\n",
    "                (\"train/iou_loss\", float(loss_components[0])),\n",
    "                (\"train/obj_loss\", float(loss_components[1])),\n",
    "                (\"train/class_loss\", float(loss_components[2])),\n",
    "                (\"train/loss\", to_cpu(loss).item())]\n",
    "            logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
    "\n",
    "            model.seen += imgs.size(0)\n",
    "\n",
    "        # #############\n",
    "        # Save progress\n",
    "        # #############\n",
    "\n",
    "        # Save model to checkpoint file\n",
    "        if epoch % args.checkpoint_interval == 0:\n",
    "            checkpoint_path = f\"checkpoints/yolov3_ckpt_{time.strftime('%Y%m%d_%H%M')}_{epoch}.pth\"\n",
    "            print(f\"---- Saving checkpoint to: '{checkpoint_path}' ----\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # ########\n",
    "        # Evaluate\n",
    "        # ########\n",
    "\n",
    "        if epoch % args.evaluation_interval == 0:\n",
    "            print(\"\\n---- Evaluating Model ----\")\n",
    "            # Evaluate the model on the validation set\n",
    "            metrics_output = _evaluate(\n",
    "                model,\n",
    "                validation_dataloader,\n",
    "                class_names,\n",
    "                img_size=model.hyperparams['height'],\n",
    "                iou_thres=args.iou_thres,\n",
    "                conf_thres=args.conf_thres,\n",
    "                nms_thres=args.nms_thres,\n",
    "                verbose=args.verbose\n",
    "            )\n",
    "\n",
    "            if metrics_output is not None:\n",
    "                precision, recall, AP, f1, ap_class = metrics_output\n",
    "                history.append({\n",
    "                    'epoch':epoch,\n",
    "                    'precision':precision.mean(),\n",
    "                    'recall':recall.mean(),\n",
    "                    'mAP':AP.mean(),\n",
    "                    'f1':f1.mean(),\n",
    "                })\n",
    "                evaluation_metrics = [\n",
    "                    (\"validation/precision\", history[-1]['precision']),\n",
    "                    (\"validation/recall\", history[-1]['recall']),\n",
    "                    (\"validation/mAP\", history[-1]['mAP']),\n",
    "                    (\"validation/f1\", history[-1]['f1'])]\n",
    "                logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
    "\n",
    "                json.dump(history, open('history.json', 'w'))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    if False:\n",
    "        d = r'/kaggle/input/pjt-faster-rcnn-20240622/checkpoints'\n",
    "        all_trained = sorted([os.path.join(d,f) for f in os.listdir(d)])\n",
    "        print(all_trained)\n",
    "        filepath_trained = all_trained[-1]\n",
    "        print(f'filepath_trained = {filepath_trained}')\n",
    "\n",
    "        pretrained_weights = filepath_trained\n",
    "\n",
    "        history = json.load(open(r'/kaggle/input/pjt-faster-rcnn-20240622/history.json', 'r'))\n",
    "        seed = -1\n",
    "    else:\n",
    "        history = []\n",
    "        pretrained_weights = None\n",
    "        seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if True: # COCO2014\n",
    "        if torch.cuda.is_available():\n",
    "            command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "                --dataset_name COCO2014\n",
    "                --data_dir /kaggle/input/coco-2014-dataset-for-yolov3/coco2014\n",
    "                --train /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/trainvalno5k.part\n",
    "                --valid /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/5k.part\n",
    "                --names /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/coco.names\n",
    "                --pretrained_weights {'/kaggle/input/datasets-yolov3/darknet53.conv.74' if pretrained_weights is None else pretrained_weights}\n",
    "                --epochs 10\n",
    "                --n_cpu 4\n",
    "                --seed {seed}\n",
    "                \"\"\")\n",
    "        else:\n",
    "            command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "                --dataset_name COCO2014\n",
    "                --data_dir C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\n",
    "                --train C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\trainvalno5k.part\n",
    "                --valid C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\5k.part\n",
    "                --names C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\coco.names\n",
    "                --pretrained_weights C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\darknet53.conv.74\n",
    "                --epochs 300\n",
    "                --n_cpu 0\n",
    "                --seed 12345\n",
    "                \"\"\")\n",
    "    else: # VOC2007\n",
    "        if torch.cuda.is_available():\n",
    "            command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "                --dataset_name VOC2007\n",
    "                --data_dir /kaggle/input/datasets-voc-all-2007/VOCdevkit/VOC2007\n",
    "                --train /kaggle/input/datasets-voc-all-2007/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt\n",
    "                --valid /kaggle/input/datasets-voc-all-2007/VOCdevkit/VOC2007/ImageSets/Main/test.txt\n",
    "                --pretrained_weights {'/kaggle/input/datasets-yolov3/darknet53.conv.74' if pretrained_weights is None else pretrained_weights}\n",
    "                --epochs 100\n",
    "                --n_cpu 4\n",
    "                --seed {seed}\n",
    "                \"\"\")\n",
    "        else:\n",
    "            command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "                --dataset_name VOC2007\n",
    "                --data_dir C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\voc2007\\VOCdevkit\\VOC2007\n",
    "                --train C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\voc2007\\VOCdevkit\\VOC2007\\ImageSets\\Main\\trainval.txt\n",
    "                --valid C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\voc2007\\VOCdevkit\\VOC2007\\ImageSets\\Main\\test.txt\n",
    "                --pretrained_weights C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\darknet53.conv.74\n",
    "                --epochs 300\n",
    "                --n_cpu 0\n",
    "                --seed 12345\n",
    "                \"\"\")        \n",
    "    print(command_args)\n",
    "\n",
    "    history = run_train(command_args, history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    df[['mAP']].plot(ax=axes[0], figsize=(12,4), logy=False)\n",
    "    df[['precision','recall','f1']].plot(ax=axes[1], figsize=(12,4), logy=False)\n",
    "    plt.show()\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     if torch.cuda.is_available():\n",
    "#         command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "#             --dataset_name COCO2014\n",
    "#             --data_dir /kaggle/input/coco-2014-dataset-for-yolov3/coco2014\n",
    "#             --valid /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/5k.part\n",
    "#             --names /kaggle/input/coco-2014-dataset-for-yolov3/coco2014/coco.names\n",
    "#             --weights /kaggle/input/datasets-yolov3/yolov3.weights\n",
    "#             --n_cpu 4\n",
    "#             \"\"\")\n",
    "#     else:\n",
    "#         command_args = re.findall('[^\\s=]+', fr\"\"\"\n",
    "#             --dataset_name COCO2014\n",
    "#             --data_dir C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\n",
    "#             --valid C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\5k.part\n",
    "#             --names C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\coco.names\n",
    "#             --weights C:\\Users\\bomso\\bomsoo1\\python\\_pytorch\\data\\COCO\\yolov3.weights\n",
    "#             --n_cpu 0\n",
    "#             \"\"\")\n",
    "#     print(command_args)\n",
    "\n",
    "#     history = run_test(command_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
